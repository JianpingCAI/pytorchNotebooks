{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test access to gpu/cuda\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#========================== Table of Contents ==========================#\n",
    "1. Basic autograd example 1               (Line 21 to 36)\n",
    "2. Basic autograd example 2               (Line 39 to 77)\n",
    "3. Loading data from numpy                (Line 80 to 83)\n",
    "4. Implementing the input pipline         (Line 86 to 113)\n",
    "5. Input pipline for custom dataset       (Line 116 to 138)\n",
    "6. Using pretrained model                 (Line 141 to 155)\n",
    "7. Save and load model                    (Line 158 to 165) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#======================= Basic autograd example 1 =======================#\n",
    "# Create tensors. These are one-dimensional tensors, i.e. scalars. They have some contant value for simplicity. x is some input,\n",
    "# w is the weight value, and b is some bias value\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# Build a linear computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients through backpropagation, this does the chain rule, working backwards through the graph.\n",
    "# Pytorch makes this very easy.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 ; the gradient of the y value with respect to x is just 2, as w = 2\n",
    "print(w.grad)    # w.grad = 1 ; the gradient of the y value with respect to w is just 1, as x = 1\n",
    "print(b.grad)    # b.grad = 1 ; the gradient of the y value with respect to w is just 1, as b is a constantso its gradient is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "-0.0011 -0.2031  0.2692\n",
      " 0.5016 -0.4425 -0.0794\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b:  Parameter containing:\n",
      "-0.4432\n",
      " 0.3404\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loss:  1.7113735675811768\n",
      "dL/dw:  Variable containing:\n",
      " 0.4867 -0.7298 -0.1098\n",
      " 1.7210 -0.0865 -0.4354\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "dL/db:  Variable containing:\n",
      " 0.0029\n",
      " 0.7094\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loss after 1 step optimization:  1.6673835515975952\n"
     ]
    }
   ],
   "source": [
    "#======================== Basic autograd example 2 =======================#\n",
    "# Create tensors for input and output values. Here we have two-dimensional matrices i.e. tensors that are 5x3 and 5x2 in size \n",
    "# x is a matrix 5 rows long and 3 columns wide. y is a matrix 5 rows long and 2 columns wide. 5 is the batch size\n",
    "# So for any given sample within a batch the following is done: (2x3 weight matrix)x(3x1 input matrix)+(bias matrix)=(2x1 matrix)\n",
    "x = Variable(torch.randn(5, 3))\n",
    "y = Variable(torch.randn(5,2))\n",
    "\n",
    "# Build a linear layer. This uses the syntax nn.Linear(in_features, out_features) where these are # of input/output features. \n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build Loss and Optimizer. Here we are using Mean Squared Error and Stochastic Gradient Descent respectively.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward propagation using the linear neural network object.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.data[0])\n",
    "\n",
    "# Backpropagation.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step Optimization (gradient descent).\n",
    "optimizer.step()\n",
    "\n",
    "# You can also do optimization at the low level as shown below.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after optimization. Here you can see each row of the weight matrix acts as a separate classifier\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#======================== Loading data from numpy ========================#\n",
    "a = np.array([[1,2], [3,4]])\n",
    "b = torch.from_numpy(a)      # convert numpy array to torch tensor\n",
    "c = b.numpy()                # convert torch tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#===================== Implementing the input pipline =====================#\n",
    "# Download and construct dataset.\n",
    "train_dataset = dsets.CIFAR10(root='../data/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "# Select one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data Loader (this provides queue and thread in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Your training code will be written here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===================== Input pipline for custom dataset =====================#\n",
    "# You should build custom dataset as below.\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file path or list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# Then, you can just use prebuilt torch's data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/cblythe2/.torch/models/resnet18-5c106cde.pth\n",
      "52.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "#========================== Using pretrained model ==========================#\n",
    "# Download and load pretrained resnet.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only top layer of the model.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 class scores for output is for example.\n",
    "\n",
    "# For test.\n",
    "images = Variable(torch.randn(10, 3, 256, 256)) # 10 is batch size, 3 is number of color channels for 256x256 pixel images\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())   # (10, 100) 10 is batch size, 100 is number of class scores for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================ Save and load the model ============================#\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.pkl')\n",
    "model = torch.load('model.pkl')\n",
    "\n",
    "# Save and load only the model parameters(recommended).\n",
    "torch.save(resnet.state_dict(), 'params.pkl')\n",
    "resnet.load_state_dict(torch.load('params.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
